{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUuBe0zcc19s"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDG9lRM9c-k4",
        "outputId": "12e63cc2-b06e-49fe-878d-84c33b85d832"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/REU evolution of scientific fields\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PawvfkGRLJGc"
      },
      "source": [
        "# **Data Processing and Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwZbb9bBNNRS"
      },
      "source": [
        "See Summary Statistics and Exploration notebook for discussion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifgCIdGtBhnT"
      },
      "source": [
        "#Load full dataset\n",
        "with open(os.path.join(DRIVE_PATH, 'fields_df.pkl'), 'rb') as file:\n",
        "    fields_df = pickle.load(file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwo2RdD4CPW",
        "outputId": "027e3668-62cb-4c55-cdd9-b1042c2d6aa2"
      },
      "source": [
        "#Keep only the first 200000 papers; later papers are less recent and have lower log probability\n",
        "new_fields_df = fields_df[:200000]\n",
        "\n",
        "#Drop papers that are missing abstracts/references\n",
        "new_fields_df = new_fields_df.dropna(subset=['RId', 'IA']).reset_index(drop=True)\n",
        "\n",
        "#Remove papers published prior to 1950\n",
        "new_fields_df = new_fields_df[new_fields_df['Y'] >= 1950]\n",
        "\n",
        "#Remove papers with abstracts that are less than 30 words or more than 500 words\n",
        "new_fields_df = new_fields_df[new_fields_df[\"IA\"].apply(lambda f: f.get('IndexLength')) >= 30]\n",
        "new_fields_df = new_fields_df[new_fields_df[\"IA\"].apply(lambda f: f.get('IndexLength')) <= 500]\n",
        "\n",
        "len(new_fields_df) #Leaves 59969 papers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59969"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4b2Eu_ryS_w"
      },
      "source": [
        "#Obtain vector of abstracts\n",
        "abstracts = []\n",
        "for ia in new_fields_df.IA:\n",
        "  temp = [\"\"]*ia['IndexLength']\n",
        "  for word, idxs in ia['InvertedIndex'].items():\n",
        "    for idx in idxs:\n",
        "      temp[idx] = word\n",
        "  abstracts.append(\" \".join(temp))\n",
        "\n",
        "#Add (un-inverted) abstracts as column of dataframe\n",
        "new_fields_df = new_fields_df.assign(A=abstracts)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otNvdbl5zyFS",
        "outputId": "c71420da-34b8-4e9a-baf1-32f52827967c"
      },
      "source": [
        "#For simplicity, remove all duplicate papers\n",
        "#(ie, those that have the same abstract as another paper)\n",
        "duplicates = new_fields_df.duplicated(subset=['A'], keep=False)\n",
        "new_fields_df = new_fields_df[~duplicates]\n",
        "\n",
        "len(new_fields_df) #Leaves 59384 papers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59384"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyQNZuUUylkU"
      },
      "source": [
        "#Functions to lemmatize words in a group of text \n",
        "#https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# function to convert nltk tag to wordnet tag\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "## Lemmatize abstracts and add to dataframe\n",
        "new_fields_df[\"LA\"] = [lemmatize_sentence(text) for text in new_fields_df.A]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JZMMuS_Js0E"
      },
      "source": [
        "#Save filtered dataframe\n",
        "with open(os.path.join(DRIVE_PATH, 'filtered_new_fields_df.pkl'), 'wb') as file:\n",
        "    pickle.dump(new_fields_df, file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}